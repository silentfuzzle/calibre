#!/usr/bin/env python
# vim:fileencoding=utf-8
#
# Copyright 2014 Martin Račák <martin.racak@riseup.net>
# Copyright 2011 Miroslav Vasko <zemiak@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

__license__   = 'GPL v3'
__copyright__ = '2014 Martin Račák <martin.racak@riseup.net>, 2011 Miroslav Vasko <zemiak@gmail.com>'

'''
.týždeň - iný pohľad na spoločnosť
'''

import re
from calibre import strftime
from calibre.web.feeds.news import BasicNewsRecipe
from datetime import date

class Tyzden(BasicNewsRecipe):
    title                = u'týždeň'
    __author__           = u'Martin Račák, zemiak'
    description          = 'A conservative weekly magazine.'
    publisher            = 'www.tyzden.sk'
    publication_type     = 'magazine'
    language             = 'sk'
    needs_subscription   = 'optional'
    use_embedded_content = False
    no_stylesheets       = True

    def get_browser(self):
        br = BasicNewsRecipe.get_browser(self)
        if self.username is not None and self.password is not None:
            br.open('http://www.tyzden.sk/prihlasenie.html')
            br.select_form(nr=1)
            br['user'] = self.username
            br['pass'] = self.password
            br.submit()
        return br

    today   = date.today()
    iso     = today.isocalendar()
    year    = iso[0]
    weeknum = iso[1]

    base_url_path = 'http://www.tyzden.sk/casopis/' + str(year) + '/' + str(weeknum)
    base_url = base_url_path + '.html'

    keep_only_tags = []
    keep_only_tags.append(dict(name='div', attrs={'class': 'text_area top_nofoto'}))
    keep_only_tags.append(dict(name='div', attrs={'class': 'text_block'}))

    remove_tags_after = [dict(name='div', attrs={'class': 'text_block'})]

    extra_css = '.top_nofoto h1 { text-align: left; font-size: 1.7em; }'

    def find_sections(self):
        soup = self.index_to_soup(self.base_url)
        # find cover pic
        imgdiv = soup.find('div', attrs={'class': 'foto'})
        if imgdiv is not None:
            img = imgdiv.find('img')
            if img is not None:
                self.cover_url = 'http://www.tyzden.sk/' + img['src']
        # end find cover pic

        for s in soup.findAll('a', attrs={'href': re.compile(r'rubrika/.*')}):
            yield (self.tag_to_string(s), s)

    def find_articles(self, soup):
        for art in soup.findAllNext('a'):
            if (not art['href'].startswith('casopis/' + str(self.year) + '/' + str(self.weeknum) + '/')):
                break

            url = art['href']
            title = self.tag_to_string(art)
            yield {
                    'title': title, 'url':self.base_url_path + '/' + url,
                    'date' : strftime(' %a, %d %b'),
                    }

    def parse_index(self):
        feeds = []
        for title, soup in self.find_sections():
            feeds.append((title, list(self.find_articles(soup))))

        return feeds
